{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Scrapy Cluster data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Initializing\n",
    "\n",
    "# import necessary libraries\n",
    "import os, re, fnmatch # for navigating file trees and working with strings\n",
    "import csv # for reading in CSV files\n",
    "#from glob import glob,iglob # for finding files within nested folders--compare with os.walk\n",
    "import json, pickle, csv # For saving a loading dictionaries, DataFrames, lists, etc. in JSON, pickle, and CSV formats\n",
    "from math import log10 # For calculating logarithms of dictionary counts\n",
    "from datetime import datetime # For timestamping files\n",
    "import time #, timeout_decorator # To prevent troublesome files from bottlenecking the parsing process, use timeouts\n",
    "import sys # For working with user input\n",
    "import logging # for logging output, to help with troubleshooting\n",
    "#from nltk.stem.porter import PorterStemmer # an approximate method of stemming words\n",
    "#stemmer = PorterStemmer()\n",
    "#from nltk import word_tokenize, sent_tokenize # widely used text tokenizer\n",
    "import pandas as pd # modifies data more efficiently than with a list of dicts\n",
    "#from tqdm import tqdm # For progress information over iterations, including with Pandas operations via \"progress_apply\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Set script options\n",
    "\n",
    "Debug = False # Set to \"True\" for extra progress reports while algorithms run\n",
    "notebook = True # Use different file paths depending on whether files are being accessed from shell (False) or within a Jupyter notebook (True)\n",
    "usefile = False # Set to \"True\" if loading from file a dicts_list to add to. Confirms with user input first!\n",
    "workstation = False # If working from office PC\n",
    "\n",
    "if notebook:\n",
    "    usefile = False # Prompting user for input file is only useful in command-line\n",
    "\n",
    "inline_tags = [\"b\", \"big\", \"i\", \"small\", \"tt\", \"abbr\", \"acronym\", \"cite\", \"dfn\",\n",
    "               \"em\", \"kbd\", \"strong\", \"samp\", \"var\", \"bdo\", \"map\", \"object\", \"q\",\n",
    "               \"span\", \"sub\", \"sup\"] # this list helps with eliminating junk tags when parsing HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Set directories\n",
    "\n",
    "if workstation and notebook:\n",
    "    dir_prefix = \"C:\\\\Users\\\\Jaren\\\\Documents\\\\\" # One level further down than the others\n",
    "elif notebook:\n",
    "    dir_prefix = \"/home/jovyan/work/\"\n",
    "else:\n",
    "    dir_prefix = \"/vol_b/data/\"\n",
    "\n",
    "example_page = \"https://westlakecharter.com/about/\"\n",
    "example_schoolname = \"TWENTY-FIRST_CENTURY_NM\"\n",
    "\n",
    "save_dir = dir_prefix + \"Charter-school-identities\" + os.sep + \"data\" + os.sep # Directory in which to save data files\n",
    "dicts_dir = dir_prefix + \"Charter-school-identities\" + os.sep + \"dicts\" + os.sep # Directory in which to find & save dictionary files\n",
    "temp_dir = save_dir + \"temp\" + os.sep # Directory in which to save temporary data files\n",
    "\n",
    "micro_sample13 = save_dir + \"micro-sample13_coded.csv\" # Random micro-sample of 300 US charter schools\n",
    "URL_schooldata = save_dir + \"charter_URLs_2014.csv\" # 2014 population of 6,973 US charter schools\n",
    "full_schooldata = save_dir + \"charter_merged_2014.csv\" # Above merged with PVI, EdFacts, year opened/closed\n",
    "temp_data = save_dir + \"school_parser_temp.json\" # Full_schooldata dict with output for some schools\n",
    "example_file = save_dir + \"example_file.html\" #example_folder + \"21stcenturypa.com/wp/default?page_id=27.tmp.html\"\n",
    "\n",
    "if not workstation and not notebook:\n",
    "    wget_dataloc = dir_prefix + \"wget/parll_wget/\" #data location for schools downloaded with wget in parallel (requires server access)\n",
    "    example_folder = wget_dataloc + \"TWENTY-FIRST_CENTURY_NM/\" # Random charter school folder\n",
    "    example_file = dir_prefix + \"wget/example_file.html\" #example_folder + \"21stcenturypa.com/wp/default?page_id=27.tmp.html\"\n",
    "    \n",
    "data_year = int(2014)\n",
    "    \n",
    "# Set logging options\n",
    "log_file = temp_dir + \"data_prep_\" + str(datetime.today()) + \".log\"\n",
    "logging.basicConfig(filename=log_file,level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "schooldf = pd.read_pickle(dir_prefix + \"scrapy_cluster_data\" + os.sep + \"processed_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sigh = schooldf['data'].head()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ugh:\n",
    "    def __init__(self):\n",
    "        self.counter = 0\n",
    "    def process(self, sigh):\n",
    "        total = []\n",
    "        for x in sigh:\n",
    "            total.extend([s for s in x[-1].split(\"\\n\") if len(s) > 35])\n",
    "        self.counter += 1\n",
    "        if self.counter % 50 == 0:\n",
    "            print(self.counter)\n",
    "        return \"\\n\".join(total)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "1850\n",
      "1900\n",
      "1950\n",
      "2000\n",
      "2050\n",
      "2100\n",
      "2150\n",
      "2200\n",
      "2250\n",
      "2300\n",
      "2350\n",
      "2400\n",
      "2450\n",
      "2500\n",
      "2550\n",
      "2600\n",
      "2650\n",
      "2700\n",
      "2750\n",
      "2800\n",
      "2850\n",
      "2900\n",
      "2950\n",
      "3000\n",
      "3050\n",
      "3100\n",
      "3150\n",
      "3200\n",
      "3250\n",
      "3300\n",
      "3350\n",
      "3400\n",
      "3450\n",
      "3500\n",
      "3550\n",
      "3600\n",
      "3650\n",
      "3700\n",
      "3750\n",
      "3800\n",
      "3850\n",
      "3900\n",
      "3950\n",
      "4000\n",
      "4050\n",
      "4100\n",
      "4150\n",
      "4200\n",
      "4250\n",
      "4300\n",
      "4350\n",
      "4400\n",
      "4450\n",
      "4500\n",
      "4550\n",
      "4600\n",
      "4650\n",
      "4700\n",
      "4750\n",
      "4800\n",
      "4850\n",
      "4900\n",
      "4950\n",
      "5000\n",
      "5050\n",
      "5100\n",
      "5150\n",
      "5200\n",
      "5250\n",
      "5300\n",
      "5350\n",
      "5400\n",
      "5450\n",
      "5500\n",
      "5550\n",
      "5600\n",
      "5650\n",
      "5700\n",
      "5750\n",
      "5800\n",
      "5850\n",
      "5900\n",
      "5950\n",
      "6000\n",
      "6050\n",
      "6100\n",
      "6150\n",
      "6200\n",
      "6250\n",
      "6300\n",
      "6350\n",
      "6400\n",
      "6450\n",
      "6500\n",
      "6550\n",
      "6600\n",
      "6650\n",
      "6700\n",
      "6750\n",
      "6800\n",
      "6850\n",
      "6900\n",
      "6950\n",
      "7000\n",
      "7050\n",
      "7100\n",
      "7150\n",
      "7200\n",
      "7250\n",
      "7300\n",
      "7350\n",
      "7400\n"
     ]
    }
   ],
   "source": [
    "ughh = Ugh()\n",
    "ayy = schooldf['data'].apply(ughh.process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words = 'english', max_features=5000)\n",
    "transformed = vectorizer.fit_transform(ayy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FactorAnalysis(copy=True, iterated_power=3, max_iter=1000, n_components=10,\n",
       "        noise_variance_init=None, random_state=0, svd_method='randomized',\n",
       "        tol=0.01)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factoranal = FactorAnalysis(n_components=10)\n",
    "factoranal.fit(transformed.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1872,  777,  919, 1023, 1896, 1895, 1022, 2892,  387, 1024, 3384,\n",
       "       4265, 2890, 3390,  773, 1871, 4258,  778, 1021, 4303, 4487, 1870,\n",
       "        775, 2847,  954, 1353, 2298, 1869, 1867,  386])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argpartition(factoranal.components_[1], -30)[-30:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['begin', 'vtimezone', 'daylight', 'freq', 'rrule', 'tzoffsetto', 'dtstart', 'byday', 'bymonth', 'tzoffsetfrom']\n",
      "['fbidi', 'dbch', 'hich', 'af1', 'lsdlocked0', 'af0', 'rtlch', 'fcs1', 'fcs0', 'ltrch']\n",
      "['las', '504', 'según', 'en', 'la', 'título', 'públicas', 'enmienda', 'amended', 'harmony']\n",
      "['district', 'content', 'access', 'use', 'school', 'data', 'user', 'service', 'loop', 'information']\n",
      "['1964', 'según', '210', 'san', 'enmienda', 'ley', 'antonio', 'título', 'saisd', 'la']\n",
      "['ih', '3796', '78216', '12500', 'suite', '956', 'idea', '377', 'amended', '8000']\n",
      "['academy', 'child', 'school', 'year', 'learning', 'charter', 'students', 'education', 'children', 'student']\n",
      "['information', 'accredited', 'ajax', 'marital', 'school', 'subjected', 'excluded', 'charter', 'denied', 'nwac']\n",
      "['great', 'leaders', 'looking', 'generation', 'academies', 'hours', 'heritage', 'national', 'join', 'moral']\n",
      "['k12', 'services', 'parent', 'shall', 'student', 'district', 'policy', 'information', 'hometown', 'school']\n"
     ]
    }
   ],
   "source": [
    "vocab = vectorizer.get_feature_names()\n",
    "for c in factoranal.components_:\n",
    "    print([vocab[i] for i in np.argpartition(c, -10)[-10:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
