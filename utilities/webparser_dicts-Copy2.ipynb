{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: UTF-8\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/usr/local/lib/python3.5/dist-packages\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\"> Dictionary Analysis on HTML from `wget` run!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import os, re, fnmatch # for navigating file trees and working with strings\n",
    "import csv # for reading in CSV files\n",
    "#from glob import glob # for finding files within nested folders--compare with os.walk\n",
    "import json, pickle # For saving a loading dictionaries, etc. from file with JSON and pickle formats\n",
    "from datetime import datetime # For timestamping files\n",
    "import sys # For working with user input\n",
    "import logging # for logging output, to help with troubleshooting\n",
    "from nltk.stem.porter import PorterStemmer # an approximate method of stemming words\n",
    "stemmer = PorterStemmer()\n",
    "from nltk import word_tokenize, sent_tokenize # widely used text tokenizer\n",
    "import urllib, urllib.request # for testing pages\n",
    "from unicodedata import normalize # for cleaning text by converting unicode character encodings into readable format\n",
    "#import shelve # For working with big dictionary files without having the whole file in memory at once\n",
    "import pandas as pd # modifies data more efficiently than with a list of dicts\n",
    "from tqdm import tqdm # For progress information during involved Pandas operations\n",
    "\n",
    "# Import parser\n",
    "from bs4 import BeautifulSoup # BS reads and parses even poorly/unreliably coded HTML \n",
    "from bs4.element import Comment # helps with detecting inline/junk tags when parsing with BS\n",
    "import lxml # for fast HTML parsing with BS, compared to \"html.parser\"\n",
    "bsparser = \"lxml\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Set script options\n",
    "\n",
    "Debug = False # Set to \"True\" for extra progress reports while algorithms run\n",
    "notebook = True # Use different file paths depending on whether files are being accessed from shell (False) or within a Jupyter notebook (True)\n",
    "usefile = False # Set to \"True\" if loading from file a dicts_list to add to. Confirms with user input first!\n",
    "workstation = False # If working from office PC\n",
    "\n",
    "if notebook:\n",
    "    usefile = False # Prompting user for input file is only useful in command-line\n",
    "\n",
    "inline_tags = [\"b\", \"big\", \"i\", \"small\", \"tt\", \"abbr\", \"acronym\", \"cite\", \"dfn\",\n",
    "               \"em\", \"kbd\", \"strong\", \"samp\", \"var\", \"bdo\", \"map\", \"object\", \"q\",\n",
    "               \"span\", \"sub\", \"sup\"] # this list helps with eliminating junk tags when parsing HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Set directories\n",
    "\n",
    "\n",
    "if workstation and notebook:\n",
    "    dir_prefix = \"C:\\\\Users\\\\Jaren\\\\Documents\\\\\" # One level further down than the others\n",
    "elif notebook:\n",
    "    dir_prefix = \"/home/jovyan/work/\"\n",
    "else:\n",
    "    dir_prefix = \"/vol_b/data/\"\n",
    "    \n",
    "wget_dataloc = dir_prefix + \"wget/parll_wget/\" #data location for schools downloaded with wget in parallel (requires server access)\n",
    "\n",
    "example_page = \"https://westlakecharter.com/about/\"\n",
    "example_schoolname = \"TWENTY-FIRST_CENTURY_NM\"\n",
    "example_folder = wget_dataloc + example_schoolname\n",
    "\n",
    "\n",
    "save_dir = dir_prefix + \"Charter-school-identities\" + os.sep + \"data\" + os.sep # Directory in which to save data files\n",
    "dicts_dir = dir_prefix + \"Charter-school-identities\" + os.sep + \"dicts\" + os.sep # Directory in which to find & save dictionary files\n",
    "temp_dir = save_dir + \"temp\" + os.sep # Directory in which to save temporary data files\n",
    "\n",
    "micro_sample13 = save_dir + \"micro-sample13_coded.csv\" # Random micro-sample of 300 US charter schools\n",
    "URL_schooldata = save_dir + \"charter_URLs_2014.csv\" # 2014 population of 6,973 US charter schools\n",
    "full_schooldata = save_dir + \"charter_merged_2014.csv\" # Above merged with PVI, EdFacts, year opened/closed\n",
    "temp_data = save_dir + \"school_parser_temp.json\" # Full_schooldata dict with output for some schools\n",
    "example_file = save_dir + \"example_file.html\" #example_folder + \"21stcenturypa.com/wp/default?page_id=27.tmp.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logging options\n",
    "log_file = temp_dir + \"dict_parsing_\" + str(datetime.today()) + \".log\"\n",
    "logging.basicConfig(filename=log_file,level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set input file, if any\n",
    "if usefile and not notebook:\n",
    "    print(\"\\nWould you like to load from file a list of dictionaries to add to? (Y/N)\")\n",
    "    answer = input()\n",
    "    if answer == \"Y\":\n",
    "        print(\"Please indicate file path for dictionary list file.\")\n",
    "        answer2 = input()\n",
    "        if os.path.exists(answer2):\n",
    "            input_file = answer2\n",
    "            usefile = True\n",
    "        else:\n",
    "            print(\"Invalid file path. Aborting script.\")\n",
    "            sys.exit()\n",
    "\n",
    "    elif answer == \"N\":\n",
    "        print(\"OK! This script will create a new file for this list of dictionaries.\")\n",
    "        usefile = False\n",
    "    \n",
    "    else:\n",
    "        print(\"Response not interpretable. Aborting script.\")\n",
    "        sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Define (non-parsing) helper functions\n",
    "\n",
    "def get_vars(data):\n",
    "    \"\"\"Defines variable names based on the data source called.\"\"\"\n",
    "    \n",
    "    if data==URL_schooldata:\n",
    "        URL_variable = \"TRUE_URL\"\n",
    "        NAME_variable = \"SCH_NAME\"\n",
    "        ADDR_variable = \"ADDRESS\"\n",
    "        \n",
    "    elif data==full_schooldata:\n",
    "        URL_variable = \"SCH_NAME\" # Work-around until URLs merged into full data file\n",
    "        NAME_variable = \"SCH_NAME\"\n",
    "        ADDR_variable = \"ADDRESS14\"\n",
    "    \n",
    "    elif data==micro_sample13:\n",
    "        URL_variable = \"URL\"\n",
    "        NAME_variable = \"SCHNAM\"\n",
    "        ADDR_variable = \"ADDRESS\"\n",
    "    \n",
    "    else:\n",
    "        try:\n",
    "            print(\"Error processing variables from data file \" + str(data) + \"!\")\n",
    "        except Exception as e:\n",
    "            print(\"ERROR: No data source established!\\n\")\n",
    "            print(e)\n",
    "    \n",
    "    return(URL_variable,NAME_variable,ADDR_variable)\n",
    "\n",
    "\n",
    "def tag_visible(element):\n",
    "    \"\"\"Returns false if a web element has a non-visible tag, \n",
    "    i.e. one site visitors wouldn't actually read--and thus one we don't want to parse\"\"\"\n",
    "    \n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def webtext_from_files(datalocation):\n",
    "    \"\"\"Concatenate and return a single string from all webtext (with .txt format) in datalocation\"\"\"\n",
    "    \n",
    "    string = \"\"\n",
    "    for root, dirs, files in os.walk(datalocation):\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):\n",
    "                fileloc = open(datalocation+file, \"r\")\n",
    "                string = string + (fileloc.read())\n",
    "    return string\n",
    "\n",
    "\n",
    "def remove_spaces(file_path):\n",
    "    \"\"\"Remove spaces from text file at file_path\"\"\"\n",
    "    \n",
    "    words = [x for x in open(file_path).read().split() if x != \"\"]\n",
    "    text = \"\"\n",
    "    for word in words:\n",
    "        text += word + \" \"\n",
    "    return text\n",
    "\n",
    "\n",
    "def write_errors(error_file, error1, error2, error3, file_count):\n",
    "    \"\"\"Writes to error_file three binary error flags derived from parse_school(): \n",
    "    duplicate_flag, parse_error_flag, wget_fail_flag, and file_count.\"\"\"\n",
    "    \n",
    "    with open(error_file, 'w') as file_handler:\n",
    "        file_handler.write(\"duplicate_flag {}\\n\".format(int(error1)))\n",
    "        file_handler.write(\"parse_error_flag {}\\n\".format(int(error2)))\n",
    "        file_handler.write(\"wget_fail_flag {}\\n\".format(int(error3)))\n",
    "        file_handler.write(\"file_count {}\".format(int(file_count)))\n",
    "        return\n",
    "    \n",
    "\n",
    "def write_counts(file_path, names_list, counts_list):\n",
    "    \"\"\"Writes to file_path the input dict_count names (a list) and counts (another list).\n",
    "    Assumes these two lists have same length and are in same order--\n",
    "    e.g., names_list[0]=\"ess_count\" and counts_list[0]=ess_count.\"\"\"\n",
    "    \n",
    "    with open(file_path, 'w') as file_handler:\n",
    "        for tup in zip(names_list,counts_list): # iterate over zipped list of tuples\n",
    "            if tup != list(zip(names_list,counts_list))[-1]:\n",
    "                file_handler.write(\"{} {}\\n\".format(tup[0],tup[1]))\n",
    "            else:\n",
    "                file_handler.write(\"{} {}\".format(tup[0],tup[1]))\n",
    "        return\n",
    "\n",
    "    \n",
    "def write_list(file_path, textlist):\n",
    "    \"\"\"Writes textlist to file_path. Useful for recording output of parse_school().\"\"\"\n",
    "    \n",
    "    with open(file_path, 'w') as file_handler:\n",
    "        for elem in textlist:\n",
    "            file_handler.write(\"{}\\n\".format(elem))\n",
    "        return\n",
    "    \n",
    "\n",
    "def load_list(file_path):\n",
    "    \"\"\"Loads list into memory. Must be assigned to object.\"\"\"\n",
    "    \n",
    "    textlist = []\n",
    "    with open(file_path) as file_handler:\n",
    "        line = file_handler.readline()\n",
    "        while line:\n",
    "            textlist.append(line)\n",
    "            line = file_handler.readline()\n",
    "    return textlist\n",
    "\n",
    "        \n",
    "def save_datafile(data, file, thismode):\n",
    "    \"\"\"BROKEN for saving to CSV Pandas DataFrames (only saves header) and lists of dicts (only saves keys).\n",
    "    Saves data to file using JSON, pickle, or CSV format (whichever was specified).\n",
    "    Works with Pandas DataFrames or other objects, e.g. a list of dictionaries.\n",
    "    Deletes file first to reduce risk of data duplication.\"\"\"\n",
    "    \n",
    "    file = str(file)\n",
    "    thismode = str(thismode)\n",
    "    \n",
    "    try:\n",
    "        if os.path.exists(file):\n",
    "            os.remove(file) # Delete file first to reduce risk of data duplication\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        if thismode.upper()==\"JSON\" or thismode.upper()==\".JSON\":\n",
    "            if not file.endswith(\".json\"):\n",
    "                file += \".json\"\n",
    "            \n",
    "            if type(data)==\"pandas.core.frame.DataFrame\":\n",
    "                data.to_json(file)\n",
    "            \n",
    "            else:\n",
    "                with open(file, 'w') as outfile:\n",
    "                    json.dump(data, outfile, encoding=\"utf-8\")\n",
    "            \n",
    "            #print(\"Data saved to \" + file + \"!\")\n",
    "\n",
    "        elif thismode.lower()==\"pickle\" or thismode.lower()==\".pickle\":\n",
    "            if not file.endswith(\".pickle\"):\n",
    "                file += \".pickle\"\n",
    "                \n",
    "            if type(data)==\"pandas.core.frame.DataFrame\":\n",
    "                data.to_pickle(file, encoding=\"utf-8\")\n",
    "                \n",
    "            else:\n",
    "                with open(file, \"wb\") as outfile:\n",
    "                    pickle.dump(data, outfile, encoding=\"utf-8\")\n",
    "                    \n",
    "            #print(\"Data saved to \" + file + \"!\")\n",
    "                \n",
    "        elif thismode.upper()==\"CSV\" or thismode.upper()==\".CSV\":\n",
    "            if not file.endswith(\".csv\"):\n",
    "                file += \".csv\"\n",
    "                \n",
    "            if type(data)==\"pandas.core.frame.DataFrame\":\n",
    "                if os.path.exists(file): # If file already exists, assume we are appending to it (with same column names)\n",
    "                    data.to_csv(file,mode=\"a\",index=False,sep=\"\\t\",header=False,encoding=\"utf-8\")\n",
    "                else: # If file doesn't exist, create it\n",
    "                    data.to_csv(file,mode=\"w\",index=False,sep=\"\\t\",header=data.columns.values,encoding=\"utf-8\")\n",
    "                \n",
    "            else:\n",
    "                with open(file, \"w\") as outfile:\n",
    "                    wr = csv.writer(outfile)\n",
    "                    wr.writerows(data)\n",
    "                \n",
    "            #print(\"Data saved to \" + file + \"!\")\n",
    "\n",
    "        else:\n",
    "            print(\"ERROR! Improper arguments. Please include: data object to save (Pandas DataFrames OK), file path, and file format ('JSON', 'pickle', or 'CSV').\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"Failed to save to \" + str(file) + \" into memory using \" + str(thismode) + \" format. Please check arguments (data, file, file format) and try again.\")\n",
    "        print(e)\n",
    "    \n",
    "\n",
    "def load_datafile(file):\n",
    "    \"\"\"Loads dicts_list (or whatever) from file, using either JSON or pickle format. \n",
    "    The created object should be assigned when called.\"\"\"\n",
    "    \n",
    "    file = str(file)\n",
    "    \n",
    "    if file.lower().endswith(\".json\"):\n",
    "        with open(file,'r') as infile:\n",
    "            var = json.load(infile)\n",
    "    \n",
    "    if file.lower().endswith(\".pickle\"):\n",
    "        with open(file,'rb') as infile:\n",
    "            var = pickle.load(infile)\n",
    "        \n",
    "    #print(file + \" successfully loaded!\")\n",
    "    return var\n",
    "\n",
    "\n",
    "def load_dict(custom_dict, file_path):\n",
    "    \"\"\"Loads in a dictionary. Adds each entry from the dict at file_path to the defined set custom_dict (the input), \n",
    "    which can also be an existing dictionary. This allows the creation of combined dictionaries!\"\"\"\n",
    "\n",
    "    with open(file_path) as file_handler:\n",
    "        line = file_handler.readline()\n",
    "        while line:\n",
    "            custom_dict.add(stemmer.stem(line.replace(\"\\n\", \"\"))) # Add line after stemming dictionary entries and eliminating newlines\n",
    "            line = file_handler.readline() # Look for anything else in that line, add that too\n",
    "    return custom_dict\n",
    "\n",
    "\n",
    "def list_files(folder_path, extension):\n",
    "    \"\"\"Outputs a list of every file in folder_path or its subdirectories that has a specified extension.\n",
    "    Prepends specified extension with '.' if it doesn't start with it already.\n",
    "    If no extension is specified, it just returns all files in folder_path.\"\"\"\n",
    "    \n",
    "    matches = []\n",
    "    if extension:\n",
    "        extension = str(extension) # Coerce to string, just in case\n",
    "    \n",
    "    if extension and not extension.startswith(\".\"):\n",
    "        extension = \".\" + extension\n",
    "    \n",
    "    for dirpath,dirnames,filenames in os.walk(folder_path):\n",
    "        if extension:\n",
    "            for filename in fnmatch.filter(filenames, \"*\" + extension): # Use extension to filter list of files\n",
    "                matches.append(os.path.join(dirpath,filename))\n",
    "        else:\n",
    "                matches.append(os.path.join(dirpath,filename)) # If no extension, just take all files\n",
    "    return matches\n",
    "\n",
    "\n",
    "def has_html(folder_path):\n",
    "    \"\"\"Simple function that counts .html files and returns a binary:\n",
    "    'True' if a specified folder has any .html files in it, 'False' otherwise.\"\"\"\n",
    "    \n",
    "    html_list = []\n",
    "    for dirpath,dirnames,filenames in os.walk(folder_path):\n",
    "        for file in fnmatch.filter(filenames, \"*.html\"): # Check if any HTML files in folder_path\n",
    "            html_list.append(file)\n",
    "    \n",
    "    if len(html_list)==0:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "def convert_df(df):\n",
    "    \"\"\"Makes a Pandas DataFrame more memory-efficient through intelligent use of Pandas data types: \n",
    "    specifically, by storing columns with repetitive Python strings not with the object dtype for unique values \n",
    "    (entirely stored in memory) but as categoricals, which are represented by repeated integer values. This is a \n",
    "    net gain in memory when the reduced memory size of the category type outweighs the added memory cost of storing \n",
    "    one more thing. As such, this function checks the degree of redundancy for a given column before converting it.\"\"\"\n",
    "    \n",
    "    converted_df = pd.DataFrame() # Initialize DF for memory-efficient storage of strings (object types)\n",
    "    # TO DO: Infer dtypes of df\n",
    "    df_obj = df.select_dtypes(include=['object']).copy() # Filter to only those columns of object data type\n",
    "\n",
    "    for col in df.columns: \n",
    "        if col in df_obj: \n",
    "            num_unique_values = len(df_obj[col].unique())\n",
    "            num_total_values = len(df_obj[col])\n",
    "            if (num_unique_values / num_total_values) < 0.5: # Only convert data types if at least half of values are duplicates\n",
    "                converted_df.loc[:,col] = df[col].astype('category') # Store these columns as dtype \"category\"\n",
    "            else: \n",
    "                converted_df.loc[:,col] = df[col]\n",
    "        else:    \n",
    "            converted_df.loc[:,col] = df[col]\n",
    "                      \n",
    "    converted_df.select_dtypes(include=['float']).apply(pd.to_numeric,downcast='float')\n",
    "    converted_df.select_dtypes(include=['int']).apply(pd.to_numeric,downcast='signed')\n",
    "    \n",
    "    return converted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "school_index = next((index for (index, d) in enumerate(dicts_list) if d[\"folder_name\"] == \"Natomas_Charter_CA\"), None) # Find index of that school\n",
    "#print(school_index)\n",
    "#print(dicts_list[748][\"folder_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thispath = wget_dataloc + \"Natomas_Charter_CA/\"\n",
    "\n",
    "html_list = []\n",
    "for dirpath,dirnames,filenames in os.walk(thispath):\n",
    "    for file in fnmatch.filter(filenames, \"*.html\"): # Check if any HTML files in folder_path\n",
    "        html_list.append(file)\n",
    "\n",
    "print(len(html_list))\n",
    "#for dirpath,dirnames,filenames in os.walk(thispath):\n",
    "#    print(len([file for file in fnmatch.filter(filenames, \"*.html\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_fail_flag2(folder_name):\n",
    "    \"\"\"The web_fail_flag indicates whether the webcrawl/download operation failed to capture any .html for a particular folder_name.\n",
    "    This function sets the web_fail_flag depending on two conditions: \n",
    "    (1) Whether or not there exists a web download folder corresponding to folder_name, and\n",
    "    (2) Whether or not that folder contains at least one file with the .html extension.\"\"\"\n",
    "    \n",
    "    global wget_dataloc,dicts_list # Need access to the dictionary file\n",
    "    web_fail_flag = \"\" # make output a str to work with currently limited Pandas dtype conversion functionality\n",
    "    \n",
    "    folder_path = str(wget_dataloc) + folder_name + \"/\"\n",
    "    if (not os.path.exists(folder_path)) or (has_html(folder_path)==False):\n",
    "        web_fail_flag = str(1) # If folder doesn't exist, mark as fail and ignore when loading files\n",
    "    else:\n",
    "        web_fail_flag = str(0) # make str so can work with currently limited Pandas dtype conversion functionality\n",
    "    \n",
    "    match_index = next((index for (index, d) in enumerate(dicts_list) if d[\"folder_name\"] == folder_name), None) # Find dict index of input/folder_name\n",
    "    dicts_list[match_index]['wget_fail_flag'] = web_fail_flag # Assign output to dict entry for folder_name\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dicts_list[748][\"wget_fail_flag\"])\n",
    "print(dicts_list[748][\"folder_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set_fail_flag2(\"Natomas_Charter_CA\")\n",
    "#print(dicts_list[748][\"wget_fail_flag\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Set parsing keywords\n",
    "\n",
    "keywords = ['values', 'academics', 'skills', 'purpose',\n",
    "                       'direction', 'mission', 'vision', 'vision', 'mission', 'our purpose',\n",
    "                       'our ideals', 'ideals', 'our cause', 'curriculum','curricular',\n",
    "                       'method', 'pedagogy', 'pedagogical', 'approach', 'model', 'system',\n",
    "                       'structure','philosophy', 'philosophical', 'beliefs', 'believe',\n",
    "                       'principles', 'creed', 'credo', 'values','moral', 'history', 'our story',\n",
    "                       'the story', 'school story', 'background', 'founding', 'founded',\n",
    "                       'established','establishment', 'our school began', 'we began',\n",
    "                       'doors opened', 'school opened', 'about us', 'our school', 'who we are',\n",
    "                       'our identity', 'profile', 'highlights']\n",
    "\n",
    "mission_keywords = ['mission','vision', 'vision:', 'mission:', 'our purpose', 'our ideals', 'ideals:', 'our cause', 'cause:', 'goals', 'objective']\n",
    "curriculum_keywords = ['curriculum', 'curricular', 'program', 'method', 'pedagogy', 'pedagogical', 'approach', 'model', 'system', 'structure']\n",
    "philosophy_keywords = ['philosophy', 'philosophical', 'beliefs', 'believe', 'principles', 'creed', 'credo', 'value',  'moral']\n",
    "history_keywords = ['history', 'story','our story', 'the story', 'school story', 'background', 'founding', 'founded', 'established', 'establishment', 'our school began', 'we began', 'doors opened', 'school opened']\n",
    "about_keywords =  ['about us', 'our school', 'who we are', 'overview', 'general information', 'our identity', 'profile', 'highlights']\n",
    "\n",
    "# Create sets for each aspect and one for all keywords\n",
    "mission_keywords = set(stemmer.stem(word) for word in mission_keywords)\n",
    "curriculum_keywords = set(stemmer.stem(word) for word in curriculum_keywords)\n",
    "philosophy_keywords = set(stemmer.stem(word) for word in philosophy_keywords)\n",
    "history_keywords = set(stemmer.stem(word) for word in history_keywords)\n",
    "about_keywords =  set(stemmer.stem(word) for word in about_keywords)\n",
    "all_keywords = set(stemmer.stem(key) for key in keywords)\n",
    "\n",
    "if Debug:\n",
    "    print(\"\\nList of keywords:\\n\", list(all_keywords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 elements of combined ideology dictionary are:\n",
      " ['abstract think', 'abstract thought', 'account', 'achievement gain', 'achievement gap', 'activi', 'adapt', 'agricult', 'anim', \"another's sho\"]\n"
     ]
    }
   ],
   "source": [
    "# ### Create dictionaries for each ideology and one for combined ideologies\n",
    "\n",
    "ess_dict, prog_dict, rit_dict, all_ideol = set(), set(), set(), set()\n",
    "all_ideol = load_dict(all_ideol, dicts_dir + \"ess_dict.txt\")\n",
    "all_ideol = load_dict(all_ideol, dicts_dir + \"prog_dict.txt\")\n",
    "ess_dict = load_dict(ess_dict, dicts_dir + \"ess_dict.txt\")\n",
    "prog_dict = load_dict(prog_dict, dicts_dir + \"prog_dict.txt\")\n",
    "rit_dict = load_dict(rit_dict, dicts_dir + \"rit_dict.txt\")\n",
    "\n",
    "logging.info(str(len(all_ideol)) + \"entries loaded into the combined ideology dictionary.\")\n",
    "list_dict = list(all_ideol)\n",
    "list_dict.sort(key = lambda x: x.lower())\n",
    "print(\"First 10 elements of combined ideology dictionary are:\\n\", list_dict[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ### Define list of tuples: keywords lists and their titles, for dictionary analyses\n",
    "\n",
    "titles_list = (\"mission\",\"curriculum\",\"philosophy\",\"history\",\"about\",\"ideology\",\"keywords\")\n",
    "keysnames_tupzip = zip((mission_keywords,curriculum_keywords,philosophy_keywords,history_keywords,about_keywords,\\\n",
    "                              all_ideol,all_keywords), titles_list)\n",
    "\n",
    "dictsnames_list = (\"ess\", \"prog\", \"rit\", \"all_ideol\")\n",
    "dictsnames_tupzip = zip((ess_dict,prog_dict,rit_dict,all_ideol), dictsnames_list)\n",
    "\n",
    "if Debug:\n",
    "    print(list(keysnames_tupzip))\n",
    "    print()\n",
    "    print(list(dictsnames_tupzip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Define parsing helper functions\n",
    "\n",
    "def parsefile_by_tags(HTML_file):\n",
    "    \n",
    "    \"\"\"Cleans HTML by removing inline tags, ripping out non-visible tags, \n",
    "    replacing paragraph tags with a random string, and finally using this to separate HTML into chunks.\n",
    "    Reads in HTML from storage using a given filename, HTML_file.\"\"\"\n",
    "\n",
    "    random_string = \"\".join(map(chr, os.urandom(75))) # Create random string for tag delimiter\n",
    "    soup = BeautifulSoup(open(HTML_file), \"html5lib\")\n",
    "    \n",
    "    [s.extract() for s in soup(['style', 'script', 'head', 'title', 'meta', '[document]'])] # Remove non-visible tags\n",
    "    for it in inline_tags:\n",
    "        [s.extract() for s in soup(\"</\" + it + \">\")] # Remove inline tags\n",
    "    \n",
    "    visible_text = soup.getText(random_string).replace(\"\\n\", \"\") # Replace \"p\" tags with random string, eliminate newlines\n",
    "    # Split text into list using random string while also eliminating tabs and converting unicode to readable text:\n",
    "    visible_text = list(normalize(\"NFKC\",elem.replace(\"\\t\",\"\")) for elem in visible_text.split(random_string))\n",
    "    # TO DO: Eliminate anything with a '\\x' in it (after splitting by punctuation)\n",
    "    visible_text = list(filter(lambda vt: vt.split() != [], visible_text)) # Eliminate empty elements\n",
    "    # Consider joining list elements together with newline in between by prepending with: \"\\n\".join\n",
    "\n",
    "    return(visible_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Debug:\n",
    "    example_textlist = parsefile_by_tags(example_file)\n",
    "    print(\"Output of parsefile_by_tags:\\n\\n\", example_textlist, \"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example_textlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Define dictionary matching helper functions\n",
    "\n",
    "def dict_count(text_list, custom_dict):\n",
    "    \n",
    "    \"\"\"Performs dictionary analysis, returning number of dictionary hits found.\n",
    "    Removes punctuation and stems the phrase being analyzed. \n",
    "    Compatible with multiple-word dictionary elements.\"\"\"\n",
    "    \n",
    "    counts = 0 # number of matches between text_list and custom_dict\n",
    "    dictless_list = [] # Updated text_list with dictionary hits removed\n",
    "    max_entry_length = max([len(entry.split()) for entry in custom_dict]) # Get length (in words) of longest entry in combined dictionary\n",
    "    \n",
    "    for chunk in text_list: # chunk may be several sentences or possibly paragraphs long\n",
    "        chunk = re.sub(r'[^\\w\\s]', '', chunk) # Remove punctuation with regex that keeps only letters and spaces\n",
    "\n",
    "        # Do dictionary analysis for word chunks of lengths max_entry_length down to 1, removing matches each time.\n",
    "        # This means longer dict entries will get removed first, useful in case they contain smaller entries.\n",
    "        for length in range(max_entry_length, 0, -1):\n",
    "            dictless_chunk,len_counts = dict_match_len(chunk,custom_dict,length)\n",
    "            dictless_list.append(dictless_chunk)\n",
    "            counts += len_counts\n",
    "    \n",
    "    return dictless_list,int(counts)\n",
    "\n",
    "\n",
    "def dict_match_len(phrase, custom_dict, length):\n",
    "    \n",
    "    \"\"\"Helper function to dict_match. \n",
    "    Returns # dictionary hits and updated copy of phrase with dictionary hits removed. \n",
    "    Stems phrases before checking for matches.\"\"\"\n",
    "    \n",
    "    hits_indices, counts = [], 0\n",
    "    splitted_phrase = phrase.split()\n",
    "    if len(splitted_phrase) < length:\n",
    "        return phrase, 0 # If text chunk is shorter than length of dict entries being matched, don't continue.\n",
    "    \n",
    "    for i in range(len(splitted_phrase) - length + 1):\n",
    "        to_stem = \"\"\n",
    "        for j in range(length):\n",
    "            to_stem += splitted_phrase[i+j] + \" \" # Builds chunk of 'length' words\n",
    "        stemmed_word = stemmer.stem(to_stem[:-1]) # stem chunk\n",
    "        if stemmed_word in custom_dict:\n",
    "            hits_indices.append(i) # Store the index of the word that has a dictionary hit\n",
    "            counts += 1\n",
    "            #print(stemmed_word)\n",
    "                \n",
    "    # Iterate through list of matching word indices and remove the matches\n",
    "    for i in range(len(hits_indices)-1, -1, -1):\n",
    "        splitted_phrase = splitted_phrase[:hits_indices[i]] + \\\n",
    "        splitted_phrase[hits_indices[i] + length:]\n",
    "    modified_phrase = \"\"\n",
    "    for sp in splitted_phrase: # Rebuild the modified phrase, with matches removed\n",
    "        modified_phrase += sp + \" \"\n",
    "    return modified_phrase[:-1], counts\n",
    "\n",
    "                  \n",
    "# @timeout_decorator.timeout(20, use_signals=False)\n",
    "def dictmatch_file_helper(file, listlists, allmatch_count):\n",
    "    \"\"\"Counts number of matches in file for each list of terms given, and also collects the terms not matched.\n",
    "    listlists is a list of lists, each list containing:\n",
    "    a list of key terms--e.g., for dictsnames_biglist, currently essentialism, progressivism, ritualism, and all three combined (ess_dict, prog_dict, rit_dict, all_dicts);\n",
    "    the variables used to store the number of matches for each term lit (e.g., ess_count, prog_count, rit_count, alldict_count); \n",
    "    and the not-matches--that is, the list of words leftover from the file after all matches are removed (e.g., ess_dictless, prog_dictless, rit_dictless, alldict_dictless). \"\"\"         \n",
    "    \n",
    "    for i in range(len(dictsnames_biglist)): # Iterate over dicts to find matches with parsed text of file\n",
    "        # For dictsnames_list, dicts are: (ess_dict, prog_dict, rit_dict, alldict_count); count_names are: (ess_count, prog_count, rit_count, alldict_count); dictless_names are: (ess_dictless, prog_dictless, rit_dictless, alldict_dictless)\n",
    "        # adict,count_name,dictless_name = dictsnames_tupzip[i]\n",
    "        dictless_add,count_add = dict_count(parsed_pagetext,listlists[i][0])\n",
    "        listlists[i][1] += count_add\n",
    "        listlists[i][2] += dictless_add\n",
    "        allmatch_count += count_add\n",
    "        \n",
    "        #print(\"Discovered \" + str(count_add) + \" matches for \" + str(file) + \\\n",
    "         #            \", a total thus far of \" + str(allmatch_count) + \" matches...\")\n",
    "                  \n",
    "    return listlists,allmatch_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if Debug:\n",
    "    print(\"\\nOutput of dict_count with ideology dict:\\n\\n\", dict_count(example_textlist,all_ideol), \"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dict_page(pagetext_list, keyslist):\n",
    "    \n",
    "    \"\"\"Filters webtext of a given .html page, which is parsed and in list format, to only those strings \n",
    "    within pagetext_list containing an element (word or words) of inputted keyslist. \n",
    "    Returns list filteredtext wherein each element has original case (not coerced to lower-case).\"\"\"\n",
    "    \n",
    "    filteredtext = [] # Initialize empty list to hold strings of page\n",
    "    \n",
    "    for string in pagetext_list:\n",
    "        lowercasestring = str(string).lower() # lower-case string...\n",
    "        dict_list = [key.lower() for key in list(keyslist)] # ...compared with lower-case element of keyslist\n",
    "        for key in dict_list:\n",
    "            if key in lowercasestring and key in lowercasestring.split(' '): # Check that the word is the whole word not part of another one\n",
    "                filteredtext.append(string)\n",
    "\n",
    "    return filteredtext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Debug:\n",
    "    print(\"Output of filter_dict_page:\\n\\n\", filter_dict_page(example_textlist, all_keywords), \"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_keycount(folder_path): \n",
    "    \n",
    "    \"\"\"NOT USED.\n",
    "    Filters webtext for a given school to only those text chunks containing specified keywords.\n",
    "    Categorizes each block of text by scoring based on keyword count, using already-defined lists of keywords per category:\n",
    "    mission, philosophy, curriculum, history, \"about\"/general self-description, combined ideology, and all keywords.\"\"\"\n",
    "    \n",
    "    # TO DO: Fix this function! And compare speed with that of filter_dict_page() above, especially for longer pages.\n",
    "    \n",
    "    # Initialize keyword lists to count over (must be defined outside function)\n",
    "    global mission_keywords,curriculum_keywords,philosophy_keywords,history_keywords,about_keywords,all_ideol,all_keywords\n",
    "    mission_list,curriculum_list,philosophy_list,history_list,about_list,ideol_list,keys_list, = [],[],[],[],[],[],[]\n",
    "    \n",
    "    file_list = list_files(folder_path, \".html\")\n",
    "\n",
    "    for file in tqdm(file_list, desc=\"Filtering by keys:\"):\n",
    "        try:\n",
    "            pagetext_list = parsefile_by_tags(file)\n",
    "\n",
    "            for string in pagetext_list:\n",
    "                mission_score, curriculum_score, philosophy_score, history_score, about_score, ideol_score, keys_score = 0, 0, 0, 0, 0, 0, 0\n",
    "                for word in mission_keywords:\n",
    "                    mission_score+=string.count(word)\n",
    "                    if 'mission' in string.lower():\n",
    "                        mission_score = 2\n",
    "\n",
    "                for word in curriculum_keywords:\n",
    "                    curriculum_score+=string.count(word)\n",
    "                    if 'curriculum' in string.lower():\n",
    "                        curriculum_score = 2\n",
    "\n",
    "                for word in philosophy_keywords:\n",
    "                    philosophy_score+=string.count(word)\n",
    "                    if 'philosophy' in string.lower() or 'value' in string.lower():\n",
    "                        philosophy_score = 2\n",
    "\n",
    "                for word in history_keywords:\n",
    "                    history_score+=string.count(word)\n",
    "                    if 'history' in string.lower():\n",
    "                        history_score = 2\n",
    "\n",
    "                for word in about_keywords:\n",
    "                    about_score+=string.count(word)\n",
    "                    if 'about us' in string.lower() or \"about-us\" in string.lower():\n",
    "                        about_score = 2\n",
    "\n",
    "                for word in all_ideol:\n",
    "                    ideol_score+=string.count(word)\n",
    "\n",
    "                if mission_score>=2:\n",
    "                    mission_list.append(string)\n",
    "                if curriculum_score>=2:\n",
    "                    curriculum_list.append(string)\n",
    "                if philosophy_score>=2:\n",
    "                    philosophy_list.append(string)\n",
    "                if history_score>=2:\n",
    "                    history_list.append(string)\n",
    "                if about_score>=2:\n",
    "                    about_list.append(string)\n",
    "                if ideol_score>=2:\n",
    "                    ideol_list.append(string)\n",
    "                if ((mission_score + curriculum_score + philosophy_score + about_score) >=2): \n",
    "                    keys_list.append(string) # Impute keywords counting using its ideological constitutent elements--which excludes history_score\n",
    "\n",
    "        except Exception as e:\n",
    "            if Debug:\n",
    "                print(\"    ERROR categorizing \" + str(file))\n",
    "                print(e)\n",
    "            continue\n",
    "                    \n",
    "    return mission_list, curriculum_list, philosophy_list, history_list, about_list, ideol_list, keys_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Output of filter_by_keycount:\\n\\n\", filter_by_keycount(example_folder), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mission_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_bestmatch(folder_path, custom_dict):\n",
    "    \"\"\"Parse through all .html files in folder_path, detecting matches with custom_dict,\n",
    "    to find and return the full text from the html page that has the most matches with that dictionary.\"\"\"\n",
    "    \n",
    "    # Initialization\n",
    "    file_list = list_files(folder_path, \".html\") # Get full list of file paths\n",
    "    num_pages = len(file_list) # Number of pages in school's folder\n",
    "    max_page_hits = (-1,-1) # Initialize tuple holding #hits, page number for HTML file with greatest # matches with custom_dict \n",
    "    max_weighted_score = (-1,-1) # Same as previous, but weighted by page length\n",
    "    max_hit_text,max_score_text = [],[] # Empty lists for each best matching pages\n",
    "    \n",
    "    # Parse through pages to find maximum number of hits of custom_dict on any page\n",
    "    for pagenum in tqdm(range(num_pages), desc=\"Finding best match:\"):\n",
    "        try:\n",
    "            page_dict_count,page_weighted_score = -1,-1\n",
    "            page_textlist = parsefile_by_tags(file_list[pagenum]) # Parse page with index pagenum into text list\n",
    "            \n",
    "            if len(page_textlist)==0: # If page is empty, don't bother with it\n",
    "                continue\n",
    "\n",
    "            dictless_text, page_dict_hits = dict_count(page_textlist, custom_dict) # Count matches between custom_dict and page_textlist using dict_count\n",
    "            numwords = len('\\n'.join(page_textlist).split())\n",
    "            page_weighted_score = page_dict_hits / numwords # Weight score by number of words on page\n",
    "            logging.info(\"Found\" + str(page_dict_hits) + \"for page #\" + str(pagenum) + \"and \" + str(page_dict_hits) + \"weighting for the \" + numwords + \" words on that page.\")\n",
    "\n",
    "            if page_dict_hits > max_page_hits[0]: # Compare matches for this page with overall max\n",
    "                max_page_hits = (page_dict_hits, pagenum) # If its greater, then make new page the max\n",
    "            if page_weighted_score > max_weighted_score[0]: # Same as previous two lines, but weighted by page length\n",
    "                max_weighted_score = (page_weighted_score, pagenum)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.debug(\"    ERROR counting dict matches in page #\" + str(pagenum))\n",
    "            logging.debug(str(e))\n",
    "            continue\n",
    "                    \n",
    "    logging.info(\"Number matches and index of best matching page: \" + str(max_page_hits[0]) + \" \" + str(max_page_hits[1]))\n",
    "    logging.info(\"Number matches and index of best WEIGHTED matching page: \" + str(max_weighted_score[0]) + \" \" + str(max_weighted_score[1]))\n",
    "    \n",
    "    # Use pagenum to get text for page with highest number of hits and weighted score:\n",
    "    max_hit_text = parsefile_by_tags(file_list[max_page_hits[1]])\n",
    "    max_score_text = parsefile_by_tags(file_list[max_weighted_score[1]])\n",
    "    \n",
    "    logging.info(\"Page with the highest number of dictionary hits:\\n\\n\" + str(max_hit_text))\n",
    "    logging.info(\"Page with the highest weighted score:\\n\\n\" + str(max_score_text))\n",
    "    \n",
    "    return max_hit_text,max_score_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "\n",
    "blah \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def page_stat(file_path, custom_dict):\n",
    "\n",
    "    page_textlist = parsefile_by_tags(file_path) # Parse page with index pagenum into text list\n",
    "    #print(page_textlist)\n",
    "    dictless_text, page_dict_hits = dict_count(page_textlist, custom_dict) # Count matches between custom_dict and page_textlist using dict_count\n",
    "    numwords = len('\\n'.join(page_textlist).split())\n",
    "    page_weighted_score = page_dict_hits / numwords # Weight score by number of words on page\n",
    "    \n",
    "    \n",
    "    return page_dict_hits, page_weighted_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_stat('/home/jovyan/work/wget/parll_wget/TWENTY-FIRST_CENTURY_NM/21stcenturypa.com/wp/default?p=1214.tmp.html', keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Output of dict_bestmatch for all ideologies:\\n\", dict_bestmatch(example_folder, mission_keywords), \"\\n\\n\" )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-18-42e0b521822f>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-42e0b521822f>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    mission_keywordsdef parse_school(school_dict):\u001b[0m\n\u001b[0m                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "mission_keywordsdef parse_school(school_dict):\n",
    "    \n",
    "    \"\"\"This core function parses webtext for a given school, using helper functions to run analyses and then saving multiple outputs to school_dict:\n",
    "    counts of the number of matches between all text from a school's html pages and keywords from a defined keyword list, find dict_count();\n",
    "    and text contents of those individual pages best matching such keywords, via find_best_categories (in development).\n",
    "    \n",
    "    For the sake of parsimony and manageable script calls, OTHER similar functions/scripts collect these additional outputs: \n",
    "    full (partially cleaned) webtext, by parsing webtext of each .html file (removing inline tags, etc.) within school's folder, via parsefile_by_tags();\n",
    "    filtered webtext, by keeping only those parsed text elements containing a keyword in previously defined keywords list, via filter_keywords_page();\n",
    "    and parsed webtext, having removed overlapping headers/footers common to multiple pages, via remove_overlaps().\"\"\"\n",
    "    \n",
    "    # Allow function to access these variables already defined outside the function (globally)\n",
    "    global itervar,numschools,parsed,wget_dataloc,URL_var,NAME_var,ADDR_var\n",
    "    \n",
    "    datalocation = wget_dataloc # Define path to local data storage\n",
    "    school_name, school_address, school_URL = school[NAME_var], school[ADDR_var], school[URL_var] # Define varnames\n",
    "    itervar+=1 # Count this school\n",
    "    \n",
    "    print(\"Parsing \" + str(school_name) + \", which is school #\" + str(itervar) + \" of \" + str(numschools) + \"...\")\n",
    "    \n",
    "    # Initialize variables\n",
    "    school_dict['ess_strength'],school_dict['prog_strength'] = 0.0,0.0\n",
    "    if not usefile:\n",
    "        school_dict[\"duplicate_flag\"], school_dict[\"parse_error_flag\"] = 0, 0\n",
    "    \n",
    "    # Assign folder names\n",
    "    folder_name = re.sub(\" \",\"_\",(school_name+\" \"+school_address[-8:-6]))\n",
    "    school_dict[\"folder_name\"] = folder_name\n",
    "    school_folder = datalocation + folder_name + \"/\"\n",
    "    if school_URL==school_name:\n",
    "        school_URL = folder_name # Workaround for full_schooldata, which doesn't yet have URLs\n",
    "    \n",
    "    # Check if folder exists. If not, exit function\n",
    "    if not (os.path.exists(school_folder) or os.path.exists(school_folder.lower()) or os.path.exists(school_folder.upper())):\n",
    "        print(\"!! NO DIRECTORY FOUND matching \" + str(school_folder) + \".\\n  Aborting parsing function...\\n\\n\")\n",
    "        school_dict['wget_fail_flag'] = 1\n",
    "        return\n",
    "    \n",
    "    \n",
    "    \"\"\" # Commented out until dict_bestmatch() works\n",
    "    try:\n",
    "        for keylist,title in list(keysnames_tupzip): # Names are: (\"mission\",\"curriculum\",\"philosophy\",\"history\",\"about\",\"ideology\",\"keywords\")\n",
    "            bestvar_name = title + \"_best\" # assign varname to use as dict key\n",
    "\n",
    "            school_dict[bestvar_name],school_dict[bestvar_name+\"_weighted\"] = [],[] # initialize dict key/value pair as empty string\n",
    "            best_page,best_page_weighted = dict_bestmatch(school_folder,keylist) # Find pages best corresponding to keyword category for each in keysnames_tupzip\n",
    "            school_dict[bestvar_name].extend(best_page)\n",
    "            school_dict[bestvar_name+\"_weighted\"].extend(best_page_weighted)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(\"    ERROR! Failed to find best pages while parsing webtext of \" + str(school_name))\n",
    "        print(\"    \",e)\n",
    "        \"\"\"\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        for adict,name in list(dictsnames_tupzip): # Names are: (\"ess\", \"prog\", \"rit\", \"all_ideol\")\n",
    "            dict_name = name + \"_count\"\n",
    "            school_dict[dict_name] = dict_count(school_folder,adict)[1]\n",
    "            \n",
    "        school_dict['ess_strength'] = float(school_dict['ess_count'])/float(school_dict['rit_count'])\n",
    "        school_dict['prog_strength'] = float(school_dict['prog_count'])/float(school_dict['rit_count'])\n",
    "            \n",
    "        print(\"  SUCCESS! Counted dictionary matches for \" + str(school_name) + \"...\")\n",
    "        save_to_file(dicts_list, save_dir+\"school_dictcounts_temp\", \"JSON\") # Save output so we can pick up where left off, in case something breaks before able to save final output\n",
    "        return\n",
    "        \n",
    "    except:\n",
    "        print(\"    ERROR! Failed to count number of dict matches while parsing webtext of \" + str(school_name))\n",
    "        print(\"    \",e)\n",
    "        school_dict[\"parse_error_flag\"] = 1\n",
    "        return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Preparing data to be parsed\n",
    "\n",
    "itervar = 0 # initialize iterator that counts number of schools already parsed\n",
    "parsed = [] # initialize list of URLs that have already been parsed\n",
    "dicts_list = [] # initialize list of dictionaries to hold school data\n",
    "\n",
    "# If input_file was defined by user input in beginning of script, use that to load list of dictionaries. We'll add to it!\n",
    "if usefile and not dicts_list:\n",
    "    dicts_list = load_datafile(input_file)\n",
    "    data_loc = full_schooldata # If loading data, assume we're running on full charter population\n",
    "\n",
    "else:\n",
    "    # set charter school data file and corresponding varnames:\n",
    "    \n",
    "    data_loc = full_schooldata # Run at scale using URL list of full charter population\n",
    "    # data_loc = micro_sample13 # This seems nice for debugging--except directories don't match because different data source\n",
    "        \n",
    "    # Create dict list from CSV on file, with one dict per school\n",
    "    with open(data_loc, 'r', encoding = 'Latin1') as csvfile: # open data file\n",
    "        reader = csv.DictReader(csvfile) # create a reader\n",
    "        for row in reader: # loop through rows\n",
    "            dicts_list.append(row) # append each row to the list\n",
    "        \n",
    "URL_var,NAME_var,ADDR_var = get_vars(data_loc) # get varnames depending on data source\n",
    "numschools = len(dicts_list) # Count number of schools in list of dictionaries\n",
    "        \n",
    "# Note on data structures: each row, dicts_list[i] is a dictionary with keys as column name and value as info.\n",
    "# This will be translated into pandas data frame once (rather messy) website text is parsed into consistent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for school in dicts_list:\n",
    "    school[\"folder_name\"] = re.sub(\" \",\"_\",(school[NAME_var]+\" \"+school[ADDR_var][-8:-6])) # This gives name and state separated by \"_\"\n",
    "    \n",
    "    school[\"folder_path\"] = str(wget_dataloc) + school[\"folder_name\"] + \"/\" # This temporary variable simplifies next line of code\n",
    "    \n",
    "    if (has_html(school[\"folder_path\"])==False) or not os.path.exists(school[\"folder_path\"]):\n",
    "        school['wget_fail_flag'] = str(1) # If folder doesn't exist, mark as fail and ignore when loading files\n",
    "    else:\n",
    "        school['wget_fail_flag'] = str(0) # make str so can work with currently limited Pandas dtype conversion functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(schooldf[schooldf[\"folder_name\"]==\"Effie_Kokrine_Charter_School_AK\"][[\"wget_fail_flag\",\"folder_path\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(schooldf[schooldf[\"folder_name\"]==\"Natomas_Charter_CA\"][[\"wget_fail_flag\",\"folder_path\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schooldf = pd.DataFrame.from_dict(dicts_list) # Convert dicts_list into a DataFrame\n",
    "schooldf.info()\n",
    "schooldf.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schooldf[\"wget_fail_flag\"] = schooldf[\"wget_fail_flag\"].map({\"1\":True,1:True,\"0\":False,0:False}) # Convert to binary to use as conditional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schooldf[schooldf[\"wget_fail_flag\"]==True][[\"folder_name\",\"wget_fail_flag\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schooldf.head(26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "schooldf.wget_fail_flag.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tqdm.pandas(desc=\"Rocking pandas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Run parsing algorithm on schools (requires access to webcrawl output)\n",
    "\n",
    "test_dicts = dicts_list[0] # Limit number of schools to analyze, in order to refine methods\n",
    "\n",
    "if Debug:\n",
    "    for school in test_dicts:\n",
    "        parse_school(school)\n",
    "        \n",
    "else:\n",
    "    for school in dicts_list:\n",
    "        parse_school(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out results:\n",
    "if Debug:\n",
    "    print(test_dicts[0])\n",
    "else:\n",
    "    print(dicts_list[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save output:\n",
    "if Debug:\n",
    "    dictfile = \"testing_dicts_\" + str(datetime.today().strftime(\"%Y-%m-%d\"))\n",
    "    save_to_file(test_dicts, save_dir+dictfile, \"JSON\")\n",
    "else:\n",
    "    dictfile = \"school_dicts_\" + str(datetime.today().strftime(\"%Y-%m-%d\"))\n",
    "    save_to_file(dicts_list, save_dir+dictfile, \"JSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Too many weird dependencies that I don't want to deal with. \n",
    "#### Will work with /home/jovyan/work/wget/parll_wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(list_files(example_folder, \".html\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import queue as Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = Q.PriorityQueue()\n",
    "print(q.qsize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom_dict\n",
    "\n",
    "\n",
    "def get_pages(custom_dict, folder, min_num_matches=0, min_ratio_matches=0, min_num_pages=0, min_ratio_pages=0):\n",
    "    # PQ of links. (Matches, link)\n",
    "    q = Q.PriorityQueue()\n",
    "    q_not_match = Q.PriorityQueue()\n",
    "    \n",
    "    for page in list_files(folder, \".html\"): \n",
    "        word_num, word_perc = page_stat(page, custom_dict)\n",
    "                \n",
    "        if word_num >= min_num_matches and word_perc >= min_ratio_matches: \n",
    "            q.put((-word_num, page))\n",
    "        else: \n",
    "            q_not_match.put((-word_num, page))\n",
    "    \n",
    "    lst = []\n",
    "    if q.qsize() >= min_num_pages or q.qsize()/len(list_files(folder, \".html\")) >= min_ratio_pages: \n",
    "        while not q.empty(): \n",
    "            lst.append(q.get()[1])\n",
    "    \n",
    "    if len(lst) < min_num_pages: \n",
    "\n",
    "        while not q_not_match.empty() and len(lst) < min_num_pages: \n",
    "            lst.append(q_not_match.get()[1])\n",
    "    \n",
    "    return lst\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "called\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/home/jovyan/work/wget/parll_wget/TWENTY-FIRST_CENTURY_NM/21stcenturypa.com/wp/default?cat=13%2C9&paged=3.tmp.html',\n",
       " '/home/jovyan/work/wget/parll_wget/TWENTY-FIRST_CENTURY_NM/21stcenturypa.com/wp/default?cat=9&paged=3.tmp.html',\n",
       " '/home/jovyan/work/wget/parll_wget/TWENTY-FIRST_CENTURY_NM/21stcenturypa.com/wp/default?m=201710&paged=2.tmp.html',\n",
       " '/home/jovyan/work/wget/parll_wget/TWENTY-FIRST_CENTURY_NM/21stcenturypa.com/wp/default?m=2017&paged=5.tmp.html',\n",
       " '/home/jovyan/work/wget/parll_wget/TWENTY-FIRST_CENTURY_NM/21stcenturypa.com/wp/default?p=4057.tmp.html']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_pages(keywords, example_folder, 10,0,0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
