{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These packages are necessary to run a REST API. The Wayback availability API returns JSON responses.\n",
    "import json\n",
    "import urllib\n",
    "\n",
    "# For the large-scale URL processing\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': 'google.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20181107191344/http://www.google.com/', 'timestamp': '20181107191344'}}}\n"
     ]
    }
   ],
   "source": [
    "# Any URL can go here. Requires HTTP/WWW/full address.\n",
    "requested_url = 'google.com'\n",
    "base_url = 'http://archive.org/wayback/available?url='\n",
    "constructed_url = base_url + requested_url\n",
    "data = json.load(urllib.request.urlopen(constructed_url))\n",
    "print(data) # Will output whatever happened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': 'google.com', 'timestamp': '20000530', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20000520032820/http://www.google.com:80/', 'timestamp': '20000520032820'}}}\n"
     ]
    }
   ],
   "source": [
    "# Now, checking if Google's homepage from May 30th, 2000 is available. Must be formatted YYYYMMDD.\n",
    "timestamp = '20000530'\n",
    "constructed_url = base_url + requested_url + '&timestamp=' + timestamp\n",
    "\n",
    "# Same process as before\n",
    "data = json.load(urllib.request.urlopen(constructed_url))\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': 'google.com', 'timestamp': '16000530', 'archived_snapshots': {}}\n",
      "Out of 11 URLS, 11 were found on the Wayback Machine.\n"
     ]
    }
   ],
   "source": [
    "# Here's what the none output looks like. Google wasn't invented in 1950, so this should error.\n",
    "timestamp = '16000530'\n",
    "constructed_url = base_url + requested_url + '&timestamp=' + timestamp\n",
    "\n",
    "# Same process as before\n",
    "data = json.load(urllib.request.urlopen(constructed_url))\n",
    "print(data)\n",
    "\n",
    "# The length of 'archived-snapshots' tells us if the website is available. Here, there is no explicit 'unavailable' message, but there are no snapshots.\n",
    "\n",
    "# Now, going to make a method to take care of processing any list of URLs.\n",
    "def url_processor(urls, timestamp = ''):\n",
    "    base_url = 'http://archive.org/wayback/available?url='\n",
    "    available_urls = 0 # Going to keep track of how many actually worked\n",
    "    total_urls = len(urls)\n",
    "    for url in urls:\n",
    "        # If a timestamp parameter was specified, then add it, otherwise don't consider a timestamp in the request\n",
    "        if timestamp:\n",
    "            constructed_url = base_url + url + '&timestamp=' + timestamp\n",
    "        else:\n",
    "            constructed_url = base_url + url\n",
    "        # Now will make the request and see if it is available on Wayback machine\n",
    "        data = json.load(urllib.request.urlopen(constructed_url))\n",
    "        result_length = len(data['archived_snapshots'])\n",
    "        if result_length > 0: # Meaning if anything was scraped\n",
    "            available_urls += 1 \n",
    "    print(\"Out of \"+ str(total_urls) + \" URLS, \" + str(available_urls) + \" were found on the Wayback Machine.\")\n",
    "\n",
    "\n",
    "# Now we have this fancy method, let's try it out on ten charter URLs.\n",
    "sample_charters = ['http://ayaprun.lksd.org/', 'http://www.tongassschool.org/', 'https://www.kgbsd.org/ketchikancharter', 'https://paideia.asdk12.org/', 'http://anccs.asdk12.org/', 'https://rilkeschule.asdk12.org/', 'http://highlandacademy.asdk12.org/', 'http://www.frontiercs.org/', 'http://www.winterberrycharterschool.com/', 'https://familypartnership.asdk12.org/', 'https://aquarian.asdk12.org/']\n",
    "url_processor(sample_charters, '20170504')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the method can be run on a larger list.\n",
    "df_URLs = pd.read_csv(\"../../nowdata/backups/charter_URLs_2016.csv\", low_memory = False, usecols=[\"NCESSCH\", \"URL\"]) \n",
    "#df_URLs = df_URLs[[\"NCESSCH\", \"URL\"]]\n",
    "large_URLlist = df_URLs['URL'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7400"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(large_URLlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 100 URLS, 87 were found on the Wayback Machine.\n"
     ]
    }
   ],
   "source": [
    "url_processor(large_URLlist[:100], '20181106')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
