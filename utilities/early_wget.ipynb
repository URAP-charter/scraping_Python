{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# import necessary libraries\n",
    "import os, csv\n",
    "# other options:   import os.system(wget http)   OR   import urllib.urlretrieve (this is an alternative command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#setting directories\n",
    "micro_sample_cvs = \"/Users/anhnguyen/Desktop/research/scraping_Python/micro-sample_Feb17.csv\"\n",
    "wget_folder = \"/Users/anhnguyen/Desktop/research/scraping_Python/wget_20\"\n",
    "no_dir_folder = \"/Users/anhnguyen/Desktop/research/scraping_Python/wget_20\"\n",
    "learning_wget = \"/Users/anhnguyen/Desktop/research/scraping_Python/learning_wget\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample = [] # make empty list\n",
    "with open(micro_sample_cvs, 'r', encoding = 'Windows-1252')\\\n",
    "as csvfile: # open file; the windows-1252 encoding looks weird but works for this\n",
    "    reader = csv.DictReader(csvfile) # create a reader\n",
    "    for row in reader: # loop through rows\n",
    "        sample.append(row) # append each row to the list\n",
    "        \n",
    "#note: each row, sample[i] is a dictionary with keys as column name and value as info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750 OLD CLEMSON RD, COLUMBIA, SC \n",
      " https://www.richland2.org/charterhigh/ \n",
      " {'HI11F': '1', 'AM05F': '-2', 'AS11M': '0', 'LONCOD': '-80.8626', 'LATCOD': '34.1231', 'HPALF': '0', 'WH01F': '-2', 'AM06M': '-2', 'WH04M': '-2', 'AM08M': '-2', 'TR07M': '-2', 'G04OFFRD': '2', 'G12': '31', 'G07': '-2', 'AS10M': '0', 'HP12F': '0', 'AM08F': '-2', 'HP04M': '-2', 'BL08M': '-2', 'STITLI': 'N', 'G06': '-2', 'HISP': '5', 'AS09F': '0', 'AM04F': '-2', 'WH06F': '-2', 'BL03F': '-2', 'TRUGF': '-2', 'AM02F': '-2', 'WH01M': '-2', 'BL11M': '7', 'BLPKM': '-2', 'WHALM': '8', 'CHARTAUTH1': '4002', 'BL02F': '-2', 'WH03M': '-2', 'HI07M': '-2', 'G10': '7', 'STATUS': '1', 'WGET INDEX': '1', 'HP11F': '0', 'TR01F': '-2', 'ISPWHITE': 'PS', 'WH03F': '-2', 'ASKGF': '-2', 'AM01F': '-2', 'G05OFFRD': '2', 'HPUGF': '-2', 'TR08F': '-2', 'TR12M': '1', 'MSTREE': '750 OLD CLEMSON ROAD', 'LCITY': 'COLUMBIA', 'G01': '-2', 'HI06M': '-2', 'HI05M': '-2', 'HI07F': '-2', 'TR11M': '0', 'AS11F': '0', 'MCITY': 'COLUMBIA', 'G12OFFRD': '1', 'AS07F': '-2', 'ISMEMPUP': 'PS', 'HI05F': '-2', 'AMPKM': '-2', 'G03OFFRD': '2', 'AS12M': '0', 'AM10M': '0', 'TR07F': '-2', 'TR09F': '0', 'HPKGF': '-2', 'HP08M': '-2', 'HI12F': '3', 'WH10M': '1', 'AS06M': '-2', 'TRALM': '1', 'BL05M': '-2', 'CONAME': 'RICHLAND COUNTY', 'WH08M': '-2', 'AMKGF': '-2', 'LZIP': '29229', 'G01OFFRD': '2', 'PK': '-2', 'WHITE': '21', 'KG': '-2', 'WH09M': '0', 'ASUGF': '-2', 'G09OFFRD': '1', 'AM09F': '0', 'BLUGF': '-2', 'HP05F': '-2', 'TR05F': '-2', 'BL07F': '-2', 'MSTATE': 'SC', 'WH08F': '-2', 'WHPKF': '-2', 'SMEMPUP': '2', 'TR09M': '0', 'SURVYEAR': '2013', 'WHALF': '13', 'SFLE': '2', 'MZIP4': '0', 'WH11M': '3', 'UG': '-2', 'LZIP4': '0', 'G11': '28', 'VIRTUALSTAT': 'VIRTUALNO', 'HI01F': '-2', 'HI08M': '-2', 'NCESSCH': '4.50E+11', 'HIPKF': '-2', 'HP09F': '0', 'AM11F': '0', 'WH02F': '-2', 'ISPELM': 'PS', 'TR03F': '-2', 'AM03F': '-2', 'HPALM': '0', 'SFTEPUP': '2', 'WHPKM': '-2', 'HP05M': '-2', 'AS02F': '-2', 'ASIAN': '1', 'WH11F': '6', 'AS09M': '0', 'TR05M': '-2', 'GSLO': '9', 'ASUGM': '-2', 'RECONSTY': 'N', 'NSLPSTATUS': 'M', 'HIALF': '4', 'BL01M': '-2', 'HP12M': '0', 'WH06M': '-2', 'BL10M': '1', 'WH05F': '-2', 'HP10F': '0', 'TR04F': '-2', 'HPPKF': '-2', 'TRPKF': '-2', 'G02': '-2', 'AS06F': '-2', 'AS07M': '-2', 'BLACK': '39', 'G10OFFRD': '1', 'HPKGM': '-2', 'TRALF': '2', 'SEARCH PARAMS': 'RICHLAND TWO CHARTER HIGH 750 OLD CLEMSON RD, COLUMBIA, SC', 'BLALF': '23', 'HI12M': '0', 'TRKGM': '-2', 'BLPKF': '-2', 'AS02M': '-2', 'BL06M': '-2', 'HP07M': '-2', 'AS03M': '-2', 'AM12F': '0', 'HI04M': '-2', 'HI02F': '-2', 'TITLEI': '2', 'WHUGM': '-2', 'WH02M': '-2', 'HIKGF': '-2', 'SHARED': '2', 'TOTFRL': '16', 'BL07M': '-2', 'TR': '3', 'SPFEMALE': '2', 'CDCODE': '4502', 'AMPKF': '-2', 'TITLEISTAT': '6', 'ULOCAL': '21', 'HP03F': '-2', 'HI10F': '0', 'G05': '-2', 'AM01M': '-2', 'WHUGF': '-2', 'BL04M': '-2', 'BL12F': '10', 'TR11F': '2', 'G07OFFRD': '2', 'HP02M': '-2', 'MAGNET': '2', 'SPELM': '2', 'AS08F': '-2', 'SPWHITE': '2', 'WAYBACK URL': 'https://web.archive.org/web/*/https://www.richland2.org/charterhigh', 'HP06M': '-2', 'HI10M': '1', 'WH04F': '-2', 'AS04M': '-2', 'GSHI': '12', 'AS05F': '-2', 'TRKGF': '-2', 'HP03M': '-2', 'ASALM': '0', 'WH12F': '6', 'LSTATE': 'SC', 'PACIFIC': '0', 'G04': '-2', 'AS08M': '-2', 'MZIP': '29229', 'BLUGM': '-2', 'TR10F': '0', 'BL09F': '1', 'HI02M': '-2', 'WH12M': '4', 'ASPKM': '-2', 'TR03M': '-2', 'WH05M': '-2', 'G06OFFRD': '2', 'AM02M': '-2', 'HI08F': '-2', 'AMUGF': '-2', 'TYPE': '1', 'UNION': '0', 'FTE': '1', 'BL05F': '-2', 'ASALF': '1', 'SCHNAM': 'RICHLAND TWO CHARTER HIGH', 'BLALM': '16', 'AM11M': '0', 'AS04F': '-2', 'WHKGM': '-2', 'AM07M': '-2', 'ASPKF': '-2', 'AM': '0', 'AMALF': '0', 'HIUGF': '-2', 'HPUGM': '-2', 'CHARTR': '1', 'HP09M': '0', 'PKOFFRD': '2', 'RDM #': '0.934345634', 'LEANM': 'RICHLAND 02', 'CONUM': '45079', 'BL06F': '-2', 'AS03F': '-2', 'BLKGM': '-2', 'TOTETH': '69', 'AS12F': '0', 'TR02M': '-2', 'BIES': '2', 'AM10F': '0', 'CHARTAUTH2': 'M', 'BL11F': '9', 'HIALM': '1', 'HI06F': '-2', 'BL04F': '-2', 'AM09M': '0', 'HP01M': '-2', 'WH07M': '-2', 'HIPKM': '-2', 'G02OFFRD': '2', 'HI09F': '0', 'HP06F': '-2', 'TRUGM': '-2', 'HP02F': '-2', 'HP04F': '-2', 'AM12M': '0', 'BL03M': '-2', 'BL09M': '1', 'HI03M': '-2', 'WH10F': '0', 'BLKGF': '-2', 'HIKGM': '-2', 'HP10M': '0', 'KGOFFRD': '2', 'AS01F': '-2', 'G08': '-2', 'TR02F': '-2', 'WHKGF': '-2', 'HI01M': '-2', 'TRPKM': '-2', 'ASKGM': '-2', 'G09': '3', 'SCHNO': '1554', 'ISFLE': 'PS', 'TR04M': '-2', 'TR10M': '0', 'HI03F': '-2', 'LEAID': '4503390', 'PHONE': '8034191348', 'UGOFFRD': '2', 'FIPST': '45', 'HP11M': '0', 'LSTREE': '750 OLD CLEMSON RD', 'SCHOOL ID': 'SC600', 'AMALM': '0', 'URL': 'https://www.richland2.org/charterhigh/', 'TR08M': '-2', 'SEASCH': '600', 'AM03M': '-2', 'ADDRESS': '750 OLD CLEMSON RD, COLUMBIA, SC', 'G08OFFRD': '2', 'REDLCH': '4', 'HI09M': '0', 'BL01F': '-2', 'HI11M': '0', 'AMKGM': '-2', 'TR01M': '-2', 'AS01M': '-2', 'FRELCH': '12', 'TR06M': '-2', 'AMUGM': '-2', 'AM07F': '-2', 'AS05M': '-2', 'HIUGM': '-2', 'HPPKM': '-2', 'BL02M': '-2', 'RECONSTF': '2', 'LEVEL': '3', 'HP01F': '-2', 'WH09F': '1', 'WH07F': '-2', 'ISPFEMALE': 'PS', 'AM05M': '-2', 'G11OFFRD': '1', 'HP08F': '-2', 'BL10F': '3', 'AM06F': '-2', 'HI04F': '-2', 'BL12M': '7', 'MEMBER': '69', 'AS10F': '1', 'AM04M': '-2', 'STID': '4002', 'TR12F': '0', 'ISFTEPUP': 'PS', 'HP07F': '-2', 'BL08F': '-2', 'TR06F': '-2', 'G03': '-2'}\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the contents of our list called sample--just the first entry\n",
    "print(sample[0][\"ADDRESS\"], \"\\n\", sample[0][\"URL\"], \"\\n\", sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# turning this into tuples we can use with wget!\n",
    "# first, make some empty lists\n",
    "url_list = []\n",
    "name_list = []\n",
    "terms_list = []\n",
    "\n",
    "# now let's fill these lists with content from the sample\n",
    "for school in sample:\n",
    "    url_list.append(school[\"URL\"])\n",
    "    name_list.append(school[\"SCHNAM\"])\n",
    "    terms_list.append(school[\"ADDRESS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.richland2.org/charterhigh/', 'https://www.polk.edu/lakeland-gateway-to-college-high-school/', 'https://www.nhaschools.com/schools/rivercity/Pages/default.aspx'] \n",
      " ['RICHLAND TWO CHARTER HIGH', 'POLK STATE COLLEGE COLLEGIATE HIGH SCHOOL', 'RIVER CITY SCHOLARS CHARTER ACADEMY'] \n",
      " ['750 OLD CLEMSON RD, COLUMBIA, SC', '3425 WINTER LK RD LAC1200, WINTER HAVEN, FL', '944 EVERGREEN ST, GRAND RAPIDS, MI']\n"
     ]
    }
   ],
   "source": [
    "# it's VERY important that these three lists be indexed the same, so let's check:\n",
    "print(url_list[:3], \"\\n\", name_list[:3], \"\\n\", terms_list[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! Looking at the above, the 3 lists with URLs, names, and search terms, respectively, seem to line up as we would expect: URL[0] corresponds to name[0] corresponds to terms[0], and so on. Now, let's turn these lists into tuples so we can use them for wget. To do this, we use the zip function, which ABSOLUTELY DEPENDS on the consistent indexing we have just established:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('https://www.richland2.org/charterhigh/', 'RICHLAND TWO CHARTER HIGH'), ('https://www.polk.edu/lakeland-gateway-to-college-high-school/', 'POLK STATE COLLEGE COLLEGIATE HIGH SCHOOL'), ('https://www.nhaschools.com/schools/rivercity/Pages/default.aspx', 'RIVER CITY SCHOLARS CHARTER ACADEMY')]\n",
      "\n",
      " Polk State College Collegiate High School\n"
     ]
    }
   ],
   "source": [
    "tuple_list = list(zip(url_list, name_list))\n",
    "# Let's check what these tuples look like:\n",
    "print(tuple_list[:3])\n",
    "print(\"\\n\", tuple_list[1][1].title())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These look great! Now, to use the os library's wget function, we need to manually change directory just as we would in bash/ UNIX. We also need to track which directory we are in and create a folder in which to store the data for each charter.\n",
    "#### Students: be sure to change the directory listed in these commands to one that works for you! Otherwise it will throw an error, and the wget command below won't work. ####\n",
    "\n",
    "To start, we cd into the data directory, and then us getcwd (the equivalent of pwd in bash) to check if it worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/anhnguyen/Desktop/research/scraping_Python/wget_20'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "os.chdir(wget_folder)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Wget commands\n",
    "\n",
    "Note:the trailing slash on the URL is critical – if you omit it, wget will think that papers is a file rather than a directory. x/abc/ means downloading entire x/abc/ page while x/abc means downloading abc file\n",
    "\n",
    "-r recursively download files. it will also follow any other links: if there was a link to somewhere on that page, it would follow that and download it as well. By default, -r sends wget to a depth of five sites after the first one. \n",
    "\n",
    "--no-parent wget should follow links, but not beyond the last parent directory\n",
    "\n",
    "-O  Download and Store With a Different File name\n",
    "\n",
    "-b For a huge download, put the download in background\n",
    "\n",
    "--user-agent mask the user agent by using –user-agent options and show wget like a browser\n",
    "\n",
    "--tries increase retry attempts. Default: 20 times\n",
    "\n",
    "-i Download Multiple Files / URLs. First, need to create a urls.txt file.\n",
    "\n",
    "--mirror download a full website and made available for local viewing\n",
    "\n",
    "-r -A Download only certain file types\n",
    "\n",
    "-e robots=off execute command and ignore robots meta tags and robots.txt\n",
    "\n",
    "--spider determine weather the remote file exist at the destination \n",
    "\n",
    "--domains download only only PDF files from specific domains\n",
    "\n",
    "--user --password download files from password protected sites \n",
    "(wget --user=user --password=password http://www.example.com)\n",
    "\n",
    "--http-user --http-password Download from password protected http sites\n",
    "\n",
    "credits\n",
    "http://programminghistorian.org/lessons/automated-downloading-with-wget\n",
    "https://www.gnu.org/software/wget/manual/wget.html\n",
    "http://www.techsakh.com/2015/11/06/wget-examples-in-linux/\n",
    "http://www.thegeekstuff.com/2009/09/the-ultimate-wget-download-guide-with-15-awesome-examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.chdir(learning_wget)\n",
    "\n",
    "\n",
    "#Download index file\n",
    "os.system('wget -m -w 2 --limit-rate=20k http://activehistory.ca')\n",
    "os.system('wget -r -A.pdf http://url-to-webpage-with-pdfs/')\n",
    "os.system('wget -O taglist.zip http://www.vim.org/scripts/download_script.php?src_id=7701')\n",
    "os.system('wget --user-agent=\"Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.3) Gecko/2008092416 Firefox/3.0.3\" http://activehistory.ca')\n",
    "os.system('wget  --header=\"Accept: text/html\" --user-agent=\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.8; rv:21.0) Gecko/20100101 Firefox/21.0\"  http://yahoo.com')\n",
    "# os.system('wget -O --secure-protocol=auto --no-check-certificate --execute robots=off http://www.berkeley.edu/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! Now we've got tuples and we're in the right directory to start with. We are finally ready to run wget!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k=0 # initialize this numerical variable k, which keeps track of which entry in the sample we are on.\n",
    "#testing the first 10 tuples\n",
    "tuple_test = tuple_list[:10]\n",
    "\n",
    "for tup in tuple_test:\n",
    "    k += 1 # Add one to k, so we start with 1 and increase by 1 all the way up to entry # 300\n",
    "    print(\"Capturing website data for\", (tup[1].title()) + \", which is school #\" + str(k), \"of 300...\")\n",
    "    # use the tuple to create a name for the folder\n",
    "    if k < 10: # Add two zeros to the folder name if k is less than 10 (for ease of organizing the output folders)\n",
    "        dirname = \"00\" + str(k) + \" \" + (tup[1].title())\n",
    "    elif k < 100: # Add one zero if k is less than 100\n",
    "        dirname = \"0\" + str(k) + \" \" + (tup[1].title())\n",
    "    else: # Add nothing if k>100\n",
    "        dirname = str(k) + \" \" + (tup[1].title())\n",
    "    os.chdir(wget_folder)  # cd into data directory--this is a/\n",
    "    if not os.path.exists(dirname):\n",
    "    # key step because otherwise wget puts the output into whatever folder it was previously in, can get real messy.\n",
    "        os.makedirs(dirname) # create the folder using the dirname we just made, then change into that directory\n",
    "    os.chdir(dirname)  # other options to think about for wget: --page-requisites --retry-connrefused --convert-links --wait=3\n",
    "    \n",
    "    #trying different wgets\n",
    "    if(True):\n",
    "        os.system('wget -i -r -m --level=3 --reject .mov,.MOV,.avi,.AVI,.mpg,.MPG,.mpeg,.MPEG,.mp3,.MP3,.mp4,.MP4,.png, .PNG,.gif,.GIF,.jpg,.JPG,\\\n",
    "         .jpeg,.JPEG,.pdf,.PDF,.pdf_,.PDF_,.doc,.DOC,.docx,.DOCX,.xls,.XLS,.xlsx,.XLSX,.csv,.CSV,.ppt,.PPT,.pptx,.PPTX ' +(tup[0]))\n",
    "    if(False):\n",
    "        os.system('wget -pn --mirror --random-wait  -A '+  (tup[0]))\n",
    "    if(False):\n",
    "        os.system('wget -np --no-parent --show-progress --progress=dot --recursive --level=3 --convert-links --retry-connrefused \\\n",
    "         --random-wait --no-cookies --secure-protocol=auto --no-check-certificate --execute robots=off \\\n",
    "         --user-agent=\"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:11.0) Gecko/20100101 Firefox/11.0\" \\\n",
    "          --reject .mov,.MOV,.avi,.AVI,.mpg,.MPG,.mpeg,.MPEG,.mp3,.MP3,.mp4,.MP4,.png,.PNG,.gif,.GIF,.jpg,.JPG,\\\n",
    "          .jpeg,.JPEG,.pdf,.PDF,.pdf_,.PDF_,.doc,.DOC,.docx,.DOCX,.xls,.XLS,.xlsx,.XLSX,.csv,.CSV,.ppt,.PPT,.pptx,.PPTX' + ' ' + (tup[0]))\n",
    "    if(False):\n",
    "        os.system('wget --no-parent --show-progress --progress=dot --recursive --level=3 --convert-links --retry-connrefused \\\n",
    "        --random-wait --no-cookies --secure-protocol=auto --no-check-certificate --execute robots=off \\\n",
    "        --user-agent=\"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:11.0) Gecko/20100101 Firefox/11.0\" \\\n",
    "        --reject .mov,.MOV,.avi,.AVI,.mpg,.MPG,.mpeg,.MPEG,.mp3,.MP3,.mp4,.MP4,.png,.PNG,.gif,.GIF,.jpg,.JPG,\\\n",
    "        .jpeg,.JPEG,.pdf,.PDF,.pdf_,.PDF_,.doc,.DOC,.docx,.DOCX,.xls,.XLS,.xlsx,.XLSX,.csv,.CSV,.ppt,.PPT,.pptx,.PPTX' + ' ' + (tup[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting wget: what if there's no directory hierarchy?\n",
    "Although it's not common practice, some sites have no directory hierarchy, and this makes it difficult to scrape them automatically. For instance, Chinook Montessori Charter School's home URL is 'http://www.k12northstar.org/chinook', and if you go to this site you'll find links to other website pages with URLs that are NOT nested within this home URL, e.g. 'http://www.k12northstar.org/Page/2684' and 'http://www.k12northstar.org/Page/2685'. \n",
    "\n",
    "Our problem with directory-less URL structures is that the coding strategy used above really depends on wget's \"recursive\" option, which scrapes the site you give it as well as all sites included within that. For instance, it would scrape not only www.school.com/ but also www.school.com/cafeteria and www.school.com/about_us. This won't work for sites with no directory hierarchy. It may be impossible to separate out the site pages for only the school of interest if the site directory includes multiple schools at a \"flat\" level with no hierarchy... But we won't know until we try!\n",
    "\n",
    "Let's look at coding an automated solution to deal with this problem. Thus far, I've only thought up a painstaking solution that involves feeding wget each site manually. Below is an example of that approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Grabbing #142--a school that has no directory hierarchy in its website:\u001f\u001f\n",
    "os.chdir(no_dir_folder)\n",
    "\n",
    "for url in ['http://www.k12northstar.org/chinook', 'http://www.k12northstar.org/site/Default.aspx?PageID=2678', \\\n",
    "'http://www.k12northstar.org/Page/2684', 'http://www.k12northstar.org/Page/2685', \\\n",
    "'http://www.k12northstar.org/Page/2686', 'http://www.k12northstar.org/Page/2683', \\\n",
    "'http://www.k12northstar.org/Page/2704']:\n",
    "    os.system('wget --no-parent --show-progress --progress=dot --convert-links \\\n",
    "        --no-cookies --secure-protocol=auto --no-check-certificate --execute robots=off \\\n",
    "        --user-agent=\"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:11.0) Gecko/20100101 Firefox/11.0\" \\\n",
    "        --reject .mov,.mpg,.mpeg,.mp3,.png,.gif,.jpg,.jpeg,.pdf,.pdf_,.doc,.xls,.xlsx,.csv,.ppt,.pptx' + ' ' + url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
